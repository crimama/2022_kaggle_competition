{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLLhXuxPspEK"
   },
   "source": [
    "#데이터 다운로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32753,
     "status": "ok",
     "timestamp": 1657974134307,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "FmwT6vVosxuy",
    "outputId": "d71760bd-8970-46e8-c30e-b7c201327ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/data\n",
      "Downloading tabular-playground-series-apr-2022.zip to /content/data\n",
      " 89% 153M/171M [00:05<00:00, 33.8MB/s]\n",
      "100% 171M/171M [00:05<00:00, 32.4MB/s]\n",
      "Archive:  tabular-playground-series-apr-2022.zip\n",
      "  inflating: sample_submission.csv   \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n",
      "  inflating: train_labels.csv        \n"
     ]
    }
   ],
   "source": [
    "!cp /content/drive/MyDrive/kaggle.json ./\n",
    "\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!mkdir data \n",
    "%cd data \n",
    "!kaggle competitions download -c tabular-playground-series-apr-2022\n",
    "!unzip tabular-playground-series-apr-2022.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vSgo7_ltFJs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzD1IkPBs2KU"
   },
   "outputs": [],
   "source": [
    "file_dirs = glob('*.csv')\n",
    "\n",
    "df = []\n",
    "for dir in file_dirs:\n",
    "  exec(f\"{dir.split('.')[0]}=pd.read_csv('{dir}')\")\n",
    "  df.append(dir.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1657974234163,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "LJczINE1t8Ea",
    "outputId": "e3447845-3b73-4216-ac42-f56f0fc516b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-36af35a4-5699-4cf2-869c-930f9780d072\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>subject</th>\n",
       "      <th>step</th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.196291</td>\n",
       "      <td>0.112395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329204</td>\n",
       "      <td>-1.004660</td>\n",
       "      <td>-0.131638</td>\n",
       "      <td>-0.127505</td>\n",
       "      <td>0.368702</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.963873</td>\n",
       "      <td>-0.985069</td>\n",
       "      <td>0.531893</td>\n",
       "      <td>4.751492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.447450</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.658407</td>\n",
       "      <td>0.162495</td>\n",
       "      <td>0.340314</td>\n",
       "      <td>-0.209472</td>\n",
       "      <td>-0.867176</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.301301</td>\n",
       "      <td>0.082733</td>\n",
       "      <td>-0.231481</td>\n",
       "      <td>0.454390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>0.326893</td>\n",
       "      <td>-0.694328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330088</td>\n",
       "      <td>0.473678</td>\n",
       "      <td>1.280479</td>\n",
       "      <td>-0.094718</td>\n",
       "      <td>0.535878</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.002168</td>\n",
       "      <td>0.449221</td>\n",
       "      <td>-0.586420</td>\n",
       "      <td>-4.736147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>0.523184</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976991</td>\n",
       "      <td>-0.563287</td>\n",
       "      <td>-0.720269</td>\n",
       "      <td>0.793260</td>\n",
       "      <td>0.951145</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.995665</td>\n",
       "      <td>-0.434290</td>\n",
       "      <td>1.344650</td>\n",
       "      <td>0.429241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>0.272025</td>\n",
       "      <td>1.074580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.136283</td>\n",
       "      <td>0.398579</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.560109</td>\n",
       "      <td>-0.541985</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1.055636</td>\n",
       "      <td>0.812631</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>-0.223359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558075</th>\n",
       "      <td>25967</td>\n",
       "      <td>327</td>\n",
       "      <td>55</td>\n",
       "      <td>-0.282844</td>\n",
       "      <td>-1.217437</td>\n",
       "      <td>-1.666153</td>\n",
       "      <td>0.586726</td>\n",
       "      <td>-0.930698</td>\n",
       "      <td>-0.451010</td>\n",
       "      <td>-0.651184</td>\n",
       "      <td>0.368702</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>-0.723536</td>\n",
       "      <td>-0.353909</td>\n",
       "      <td>-0.914749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558076</th>\n",
       "      <td>25967</td>\n",
       "      <td>327</td>\n",
       "      <td>56</td>\n",
       "      <td>0.130603</td>\n",
       "      <td>0.349790</td>\n",
       "      <td>-1.666153</td>\n",
       "      <td>-0.324779</td>\n",
       "      <td>0.775324</td>\n",
       "      <td>-0.332835</td>\n",
       "      <td>0.099271</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.644509</td>\n",
       "      <td>0.691407</td>\n",
       "      <td>-0.613169</td>\n",
       "      <td>-0.515772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558077</th>\n",
       "      <td>25967</td>\n",
       "      <td>327</td>\n",
       "      <td>57</td>\n",
       "      <td>-0.579598</td>\n",
       "      <td>0.429622</td>\n",
       "      <td>-1.666153</td>\n",
       "      <td>0.319469</td>\n",
       "      <td>0.308861</td>\n",
       "      <td>0.282723</td>\n",
       "      <td>-0.512750</td>\n",
       "      <td>0.012214</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-0.424133</td>\n",
       "      <td>0.716855</td>\n",
       "      <td>1.628601</td>\n",
       "      <td>0.928389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558078</th>\n",
       "      <td>25967</td>\n",
       "      <td>327</td>\n",
       "      <td>58</td>\n",
       "      <td>1.278980</td>\n",
       "      <td>1.711134</td>\n",
       "      <td>-1.522820</td>\n",
       "      <td>0.802655</td>\n",
       "      <td>-0.460541</td>\n",
       "      <td>-0.055348</td>\n",
       "      <td>2.405282</td>\n",
       "      <td>0.043511</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.283960</td>\n",
       "      <td>-0.914914</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.211424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558079</th>\n",
       "      <td>25967</td>\n",
       "      <td>327</td>\n",
       "      <td>59</td>\n",
       "      <td>-1.136012</td>\n",
       "      <td>-3.702731</td>\n",
       "      <td>-1.332820</td>\n",
       "      <td>-0.766372</td>\n",
       "      <td>-0.430027</td>\n",
       "      <td>-0.091997</td>\n",
       "      <td>-2.512750</td>\n",
       "      <td>-0.022901</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>-0.653902</td>\n",
       "      <td>-0.418516</td>\n",
       "      <td>-1.453704</td>\n",
       "      <td>-1.561381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1558080 rows × 16 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36af35a4-5699-4cf2-869c-930f9780d072')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-36af35a4-5699-4cf2-869c-930f9780d072 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-36af35a4-5699-4cf2-869c-930f9780d072');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         sequence  subject  step  sensor_00  sensor_01  sensor_02  sensor_03  \\\n",
       "0               0       47     0  -0.196291   0.112395   1.000000   0.329204   \n",
       "1               0       47     1  -0.447450   0.134454   1.000000  -0.658407   \n",
       "2               0       47     2   0.326893  -0.694328   1.000000   0.330088   \n",
       "3               0       47     3   0.523184   0.751050   1.000000   0.976991   \n",
       "4               0       47     4   0.272025   1.074580   1.000000  -0.136283   \n",
       "...           ...      ...   ...        ...        ...        ...        ...   \n",
       "1558075     25967      327    55  -0.282844  -1.217437  -1.666153   0.586726   \n",
       "1558076     25967      327    56   0.130603   0.349790  -1.666153  -0.324779   \n",
       "1558077     25967      327    57  -0.579598   0.429622  -1.666153   0.319469   \n",
       "1558078     25967      327    58   1.278980   1.711134  -1.522820   0.802655   \n",
       "1558079     25967      327    59  -1.136012  -3.702731  -1.332820  -0.766372   \n",
       "\n",
       "         sensor_04  sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "0        -1.004660  -0.131638  -0.127505   0.368702       -0.1  -0.963873   \n",
       "1         0.162495   0.340314  -0.209472  -0.867176        0.2  -0.301301   \n",
       "2         0.473678   1.280479  -0.094718   0.535878        1.4   1.002168   \n",
       "3        -0.563287  -0.720269   0.793260   0.951145       -0.3  -0.995665   \n",
       "4         0.398579   0.044877   0.560109  -0.541985       -0.9   1.055636   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1558075  -0.930698  -0.451010  -0.651184   0.368702        0.4   0.008671   \n",
       "1558076   0.775324  -0.332835   0.099271   0.122137       -0.2   0.644509   \n",
       "1558077   0.308861   0.282723  -0.512750   0.012214       -1.6  -0.424133   \n",
       "1558078  -0.460541  -0.055348   2.405282   0.043511        1.9   0.283960   \n",
       "1558079  -0.430027  -0.091997  -2.512750  -0.022901       -1.1  -0.653902   \n",
       "\n",
       "         sensor_10  sensor_11  sensor_12  \n",
       "0        -0.985069   0.531893   4.751492  \n",
       "1         0.082733  -0.231481   0.454390  \n",
       "2         0.449221  -0.586420  -4.736147  \n",
       "3        -0.434290   1.344650   0.429241  \n",
       "4         0.812631   0.123457  -0.223359  \n",
       "...            ...        ...        ...  \n",
       "1558075  -0.723536  -0.353909  -0.914749  \n",
       "1558076   0.691407  -0.613169  -0.515772  \n",
       "1558077   0.716855   1.628601   0.928389  \n",
       "1558078  -0.914914   0.364198   0.211424  \n",
       "1558079  -0.418516  -1.453704  -1.561381  \n",
       "\n",
       "[1558080 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK3fBcw31TIR"
   },
   "source": [
    "# 데이터 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiWzuGqw1T5J"
   },
   "source": [
    "## 컬럼 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C13HmsM41Urp"
   },
   "outputs": [],
   "source": [
    "train.drop(columns = ['sequence','subject','step'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlUS-mKG1eZm"
   },
   "source": [
    "## 결측치 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1657955590117,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "yQFSRy_K1g4X",
    "outputId": "060fb5ea-42d3-4b4a-a531-d643834c0999"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_00    0\n",
       "sensor_01    0\n",
       "sensor_02    0\n",
       "sensor_03    0\n",
       "sensor_04    0\n",
       "sensor_05    0\n",
       "sensor_06    0\n",
       "sensor_07    0\n",
       "sensor_08    0\n",
       "sensor_09    0\n",
       "sensor_10    0\n",
       "sensor_11    0\n",
       "sensor_12    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqYS0KlECLoH"
   },
   "source": [
    "## 이상치 확인 \n",
    "- 그냥 민맥스로 스케일링 하니 이상하다 \n",
    "- 로버스트 스케일링을 해도 되지만 이번에는 민맥스를 하고 싶다\n",
    "- 그래서 아웃라이어를 쳐낼 필요가 있을 것 같다 \n",
    "- 가장 기본적인 IQR방법으로 아웃라이어를 쳐내기로 한다 \n",
    "- 근데 그럴 필요 없이 그냥 로버스트 스케일링을 하면 될 것 같다 \n",
    "- 아니다 이상치를 처내고 안처내고 스케일링이 다르다 이상치를 직접 처내기로 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x5DNJmp-DC_"
   },
   "outputs": [],
   "source": [
    "sample = np.array(train['sensor_00'])\n",
    "scaled_sample = minmax(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1657974994976,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "iEFCbseZAoRA",
    "outputId": "17a14d54-3550-4b05-80b4-ff490d980cfd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scaled_sample,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1657975135511,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "6MdvaQmaAsvc",
    "outputId": "5859860d-947c-4cee-964e-e35e3fcf5d66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train['sensor_03'],bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70RN8pxB1jir"
   },
   "source": [
    "\n",
    "## 이상치 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zV9MdAZA-GpH"
   },
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "  min_value  = np.min(data,axis=0)\n",
    "  max_value = np.max(data,axis=0)\n",
    "  return_value = (data-min_value)/(max_value-min_value)\n",
    "  return return_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2993,
     "status": "ok",
     "timestamp": 1657976848449,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "3HAyvpZwDwe9",
    "outputId": "b3a6f065-c6dc-414c-be22-f2cfec1490e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_out(data:pd.Series):\n",
    "  iqr_3 = np.percentile(data,75)\n",
    "  iqr_1 = np.percentile(data,25)\n",
    "  data.loc[data>iqr_3] = iqr_3 \n",
    "  data.loc[data<iqr_1] = iqr_1 \n",
    "  return data \n",
    "\n",
    "columns = train.columns\n",
    "for column in tqdm(columns):\n",
    "  data = train[column]\n",
    "  data = remove_out(data)\n",
    "  train[column] = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 3047,
     "status": "ok",
     "timestamp": 1657977646272,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "NOhrI4otEDRe",
    "outputId": "460c0ada-1560-4eb9-9d82-6620bfd3a526"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for i,column in enumerate(columns):\n",
    "  plt.subplot(4,4,i+1)\n",
    "  plt.hist(train[column])\n",
    "plt.show()\n",
    "#이게 맞나.....?????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBpLAlNKLuWW"
   },
   "source": [
    "이게 맞나 싶다...??????\n",
    "일단 그냥 스케일링을 진행 해 본다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3ApVcBLL4T3"
   },
   "source": [
    "## 스케일링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8q0k2g8L6Fz"
   },
   "outputs": [],
   "source": [
    "scalied_df = minmax(np.array(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dutIFALm2GrW"
   },
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1657978266801,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "PV0pisLE2FLM",
    "outputId": "5a7f7acf-196f-46bc-f760-1179310b6c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25968, 60, 13)\n"
     ]
    }
   ],
   "source": [
    "scalied_df = scalied_df.reshape(-1,60,13)\n",
    "print(scalied_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLyZzYh92MnH"
   },
   "source": [
    "## Train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOVKPVtR2NxN"
   },
   "outputs": [],
   "source": [
    "idx = int(len(scalied_df)*0.1)\n",
    "train_df = scalied_df[:idx*8]\n",
    "valid_df = scalied_df[idx*8:idx*9]\n",
    "test_df = scalied_df[idx*9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muTSNrtz3oz8"
   },
   "source": [
    "## output 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1657978289830,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "jOGEg-lT3qYf",
    "outputId": "c53e2bfa-86f1-4ec0-b557-eada4f4efb73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25968, 1)\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(train_labels['state'])\n",
    "train_labels = np.array(train_labels.drop(columns=['sequence']))\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9grDR5FU3-eI"
   },
   "outputs": [],
   "source": [
    "train_y = train_labels[:idx*8]\n",
    "valid_y = train_labels[idx*8:idx*9]\n",
    "test_y = train_labels[idx*9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwQnv2je2i7T"
   },
   "source": [
    "# 모델 : Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6LIKrea2zjR"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Bidirectional,Conv2D\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UandG-sa3Ph4"
   },
   "outputs": [],
   "source": [
    "input_shape = train_df.shape[1:]\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwxxxvpL7Sp-"
   },
   "outputs": [],
   "source": [
    "def encoding_block(node,input_layer):\n",
    "  x = input_layer \n",
    "  x = Bidirectional(LSTM(node,return_sequences=True))(x)\n",
    "  x = layers.Normalization()(x)\n",
    "  return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175891,
     "status": "ok",
     "timestamp": 1657981227021,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "x9aMAQ8N28I9",
    "outputId": "5f6c77c6-6730-4a72-a4cf-9a4472a8575d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 60, 13)]          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 60, 256)          145408    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " normalization_3 (Normalizat  (None, 60, 256)          513       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 60, 64)           73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " normalization_4 (Normalizat  (None, 60, 64)           129       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 60, 4)            1072      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " normalization_5 (Normalizat  (None, 60, 4)            9         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 240)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 482       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221,597\n",
      "Trainable params: 220,946\n",
      "Non-trainable params: 651\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1298/1298 [==============================] - 33s 20ms/step - loss: 0.6939 - accuracy: 0.5058 - val_loss: 0.6925 - val_accuracy: 0.5027\n",
      "Epoch 2/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6929 - val_accuracy: 0.5027\n",
      "Epoch 3/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6929 - accuracy: 0.5129 - val_loss: 0.6925 - val_accuracy: 0.5027\n",
      "Epoch 4/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6915 - accuracy: 0.5266 - val_loss: 0.6919 - val_accuracy: 0.5204\n",
      "Epoch 5/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6899 - accuracy: 0.5368 - val_loss: 0.6912 - val_accuracy: 0.5331\n",
      "Epoch 6/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.6900 - accuracy: 0.5373 - val_loss: 0.6908 - val_accuracy: 0.5374\n",
      "Epoch 7/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6912 - accuracy: 0.5281 - val_loss: 0.6922 - val_accuracy: 0.5162\n",
      "Epoch 8/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6904 - accuracy: 0.5354 - val_loss: 0.6910 - val_accuracy: 0.5431\n",
      "Epoch 9/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.6898 - accuracy: 0.5381 - val_loss: 0.6909 - val_accuracy: 0.5393\n",
      "Epoch 10/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6896 - accuracy: 0.5380 - val_loss: 0.6917 - val_accuracy: 0.5250\n",
      "Epoch 11/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6893 - accuracy: 0.5375 - val_loss: 0.6906 - val_accuracy: 0.5293\n",
      "Epoch 12/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6892 - accuracy: 0.5387 - val_loss: 0.6895 - val_accuracy: 0.5408\n",
      "Epoch 13/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.6889 - accuracy: 0.5362 - val_loss: 0.6888 - val_accuracy: 0.5401\n",
      "Epoch 14/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6886 - accuracy: 0.5404 - val_loss: 0.6897 - val_accuracy: 0.5420\n",
      "Epoch 15/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6889 - accuracy: 0.5394 - val_loss: 0.6885 - val_accuracy: 0.5385\n",
      "Epoch 16/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.6881 - accuracy: 0.5346 - val_loss: 0.6879 - val_accuracy: 0.5424\n",
      "Epoch 17/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6872 - accuracy: 0.5420 - val_loss: 0.6889 - val_accuracy: 0.5354\n",
      "Epoch 18/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6883 - accuracy: 0.5345 - val_loss: 0.6884 - val_accuracy: 0.5455\n",
      "Epoch 19/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6879 - accuracy: 0.5381 - val_loss: 0.6887 - val_accuracy: 0.5378\n",
      "Epoch 20/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.6863 - accuracy: 0.5444 - val_loss: 0.6860 - val_accuracy: 0.5462\n",
      "Epoch 21/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6823 - accuracy: 0.5534 - val_loss: 0.6823 - val_accuracy: 0.5505\n",
      "Epoch 22/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6804 - accuracy: 0.5563 - val_loss: 0.6892 - val_accuracy: 0.5462\n",
      "Epoch 23/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.6847 - accuracy: 0.5468 - val_loss: 0.6853 - val_accuracy: 0.5424\n",
      "Epoch 24/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6855 - accuracy: 0.5495 - val_loss: 0.6848 - val_accuracy: 0.5555\n",
      "Epoch 25/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6825 - accuracy: 0.5571 - val_loss: 0.6911 - val_accuracy: 0.5189\n",
      "Epoch 26/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6864 - accuracy: 0.5380 - val_loss: 0.6839 - val_accuracy: 0.5416\n",
      "Epoch 27/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.6797 - accuracy: 0.5641 - val_loss: 0.6795 - val_accuracy: 0.5801\n",
      "Epoch 28/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.6695 - accuracy: 0.5939 - val_loss: 0.6703 - val_accuracy: 0.5913\n",
      "Epoch 29/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6640 - accuracy: 0.5976 - val_loss: 0.6695 - val_accuracy: 0.5994\n",
      "Epoch 30/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6522 - accuracy: 0.6184 - val_loss: 0.6279 - val_accuracy: 0.6564\n",
      "Epoch 31/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.6146 - accuracy: 0.6665 - val_loss: 0.6079 - val_accuracy: 0.6718\n",
      "Epoch 32/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.5904 - accuracy: 0.6839 - val_loss: 0.6193 - val_accuracy: 0.6649\n",
      "Epoch 33/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.5750 - accuracy: 0.7011 - val_loss: 0.5865 - val_accuracy: 0.6949\n",
      "Epoch 34/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.5600 - accuracy: 0.7084 - val_loss: 0.5817 - val_accuracy: 0.6957\n",
      "Epoch 35/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.5423 - accuracy: 0.7228 - val_loss: 0.5630 - val_accuracy: 0.7065\n",
      "Epoch 36/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.5256 - accuracy: 0.7357 - val_loss: 0.5625 - val_accuracy: 0.7184\n",
      "Epoch 37/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.5071 - accuracy: 0.7495 - val_loss: 0.5397 - val_accuracy: 0.7273\n",
      "Epoch 38/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.4941 - accuracy: 0.7602 - val_loss: 0.5390 - val_accuracy: 0.7300\n",
      "Epoch 39/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.4778 - accuracy: 0.7703 - val_loss: 0.5327 - val_accuracy: 0.7377\n",
      "Epoch 40/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.4653 - accuracy: 0.7764 - val_loss: 0.5242 - val_accuracy: 0.7458\n",
      "Epoch 41/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.4520 - accuracy: 0.7839 - val_loss: 0.5099 - val_accuracy: 0.7450\n",
      "Epoch 42/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.4361 - accuracy: 0.7951 - val_loss: 0.5732 - val_accuracy: 0.7361\n",
      "Epoch 43/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.4182 - accuracy: 0.8043 - val_loss: 0.5686 - val_accuracy: 0.7411\n",
      "Epoch 44/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.4045 - accuracy: 0.8136 - val_loss: 0.5275 - val_accuracy: 0.7211\n",
      "Epoch 45/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.3911 - accuracy: 0.8203 - val_loss: 0.5377 - val_accuracy: 0.7408\n",
      "Epoch 46/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.3767 - accuracy: 0.8292 - val_loss: 0.5431 - val_accuracy: 0.7461\n",
      "Epoch 47/100\n",
      "1298/1298 [==============================] - 24s 18ms/step - loss: 0.3536 - accuracy: 0.8402 - val_loss: 0.5906 - val_accuracy: 0.7408\n",
      "Epoch 48/100\n",
      "1298/1298 [==============================] - 24s 18ms/step - loss: 0.3354 - accuracy: 0.8509 - val_loss: 0.5789 - val_accuracy: 0.7361\n",
      "Epoch 49/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.3167 - accuracy: 0.8613 - val_loss: 0.6126 - val_accuracy: 0.7454\n",
      "Epoch 50/100\n",
      "1298/1298 [==============================] - 24s 18ms/step - loss: 0.2993 - accuracy: 0.8687 - val_loss: 0.6406 - val_accuracy: 0.7384\n",
      "Epoch 51/100\n",
      "1298/1298 [==============================] - 24s 18ms/step - loss: 0.2834 - accuracy: 0.8781 - val_loss: 0.6283 - val_accuracy: 0.7280\n",
      "Epoch 52/100\n",
      "1298/1298 [==============================] - 24s 18ms/step - loss: 0.2609 - accuracy: 0.8904 - val_loss: 0.6519 - val_accuracy: 0.7427\n",
      "Epoch 53/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.2441 - accuracy: 0.8983 - val_loss: 0.7288 - val_accuracy: 0.7338\n",
      "Epoch 54/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.2252 - accuracy: 0.9090 - val_loss: 0.6945 - val_accuracy: 0.7408\n",
      "Epoch 55/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.2095 - accuracy: 0.9137 - val_loss: 0.7646 - val_accuracy: 0.7377\n",
      "Epoch 56/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1967 - accuracy: 0.9215 - val_loss: 0.8286 - val_accuracy: 0.7361\n",
      "Epoch 57/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1738 - accuracy: 0.9299 - val_loss: 0.8231 - val_accuracy: 0.7384\n",
      "Epoch 58/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1623 - accuracy: 0.9356 - val_loss: 0.9177 - val_accuracy: 0.7311\n",
      "Epoch 59/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1468 - accuracy: 0.9406 - val_loss: 0.9832 - val_accuracy: 0.7338\n",
      "Epoch 60/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1328 - accuracy: 0.9489 - val_loss: 0.9544 - val_accuracy: 0.7408\n",
      "Epoch 61/100\n",
      "1298/1298 [==============================] - 24s 19ms/step - loss: 0.1210 - accuracy: 0.9547 - val_loss: 1.0429 - val_accuracy: 0.7304\n",
      "Epoch 62/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1138 - accuracy: 0.9578 - val_loss: 0.9997 - val_accuracy: 0.7300\n",
      "Epoch 63/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.1054 - accuracy: 0.9614 - val_loss: 1.0916 - val_accuracy: 0.7296\n",
      "Epoch 64/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.1047 - accuracy: 0.9612 - val_loss: 1.1081 - val_accuracy: 0.7369\n",
      "Epoch 65/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0868 - accuracy: 0.9684 - val_loss: 1.1928 - val_accuracy: 0.7357\n",
      "Epoch 66/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0813 - accuracy: 0.9703 - val_loss: 1.2137 - val_accuracy: 0.7427\n",
      "Epoch 67/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0812 - accuracy: 0.9709 - val_loss: 1.2704 - val_accuracy: 0.7408\n",
      "Epoch 68/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0742 - accuracy: 0.9730 - val_loss: 1.3019 - val_accuracy: 0.7396\n",
      "Epoch 69/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0748 - accuracy: 0.9743 - val_loss: 1.1524 - val_accuracy: 0.7388\n",
      "Epoch 70/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0652 - accuracy: 0.9768 - val_loss: 1.2761 - val_accuracy: 0.7331\n",
      "Epoch 71/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0644 - accuracy: 0.9787 - val_loss: 1.2912 - val_accuracy: 0.7357\n",
      "Epoch 72/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0632 - accuracy: 0.9777 - val_loss: 1.3892 - val_accuracy: 0.7311\n",
      "Epoch 73/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0645 - accuracy: 0.9775 - val_loss: 1.2822 - val_accuracy: 0.7284\n",
      "Epoch 74/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0534 - accuracy: 0.9807 - val_loss: 1.4204 - val_accuracy: 0.7404\n",
      "Epoch 75/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0603 - accuracy: 0.9790 - val_loss: 1.3900 - val_accuracy: 0.7304\n",
      "Epoch 76/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0465 - accuracy: 0.9843 - val_loss: 1.4394 - val_accuracy: 0.7473\n",
      "Epoch 77/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0472 - accuracy: 0.9844 - val_loss: 1.4518 - val_accuracy: 0.7315\n",
      "Epoch 78/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0454 - accuracy: 0.9852 - val_loss: 1.5207 - val_accuracy: 0.7388\n",
      "Epoch 79/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0461 - accuracy: 0.9850 - val_loss: 1.4886 - val_accuracy: 0.7288\n",
      "Epoch 80/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0492 - accuracy: 0.9841 - val_loss: 1.3917 - val_accuracy: 0.7234\n",
      "Epoch 81/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0437 - accuracy: 0.9866 - val_loss: 1.5523 - val_accuracy: 0.7242\n",
      "Epoch 82/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0436 - accuracy: 0.9855 - val_loss: 1.4734 - val_accuracy: 0.7361\n",
      "Epoch 83/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0398 - accuracy: 0.9869 - val_loss: 1.5656 - val_accuracy: 0.7292\n",
      "Epoch 84/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0395 - accuracy: 0.9869 - val_loss: 1.5392 - val_accuracy: 0.7415\n",
      "Epoch 85/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0413 - accuracy: 0.9859 - val_loss: 1.4968 - val_accuracy: 0.7346\n",
      "Epoch 86/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0405 - accuracy: 0.9866 - val_loss: 1.5913 - val_accuracy: 0.7377\n",
      "Epoch 87/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0404 - accuracy: 0.9870 - val_loss: 1.5425 - val_accuracy: 0.7411\n",
      "Epoch 88/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0261 - accuracy: 0.9911 - val_loss: 1.6663 - val_accuracy: 0.7381\n",
      "Epoch 89/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0363 - accuracy: 0.9879 - val_loss: 1.5954 - val_accuracy: 0.7223\n",
      "Epoch 90/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 1.5406 - val_accuracy: 0.7257\n",
      "Epoch 91/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 1.6056 - val_accuracy: 0.7361\n",
      "Epoch 92/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0332 - accuracy: 0.9889 - val_loss: 1.6418 - val_accuracy: 0.7273\n",
      "Epoch 93/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 1.5191 - val_accuracy: 0.7223\n",
      "Epoch 94/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0325 - accuracy: 0.9894 - val_loss: 1.5548 - val_accuracy: 0.7369\n",
      "Epoch 95/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0250 - accuracy: 0.9911 - val_loss: 1.7356 - val_accuracy: 0.7373\n",
      "Epoch 96/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 1.6599 - val_accuracy: 0.7338\n",
      "Epoch 97/100\n",
      "1298/1298 [==============================] - 26s 20ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 1.7424 - val_accuracy: 0.7404\n",
      "Epoch 98/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 1.6809 - val_accuracy: 0.7331\n",
      "Epoch 99/100\n",
      "1298/1298 [==============================] - 25s 19ms/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: 1.5912 - val_accuracy: 0.7400\n",
      "Epoch 100/100\n",
      "1298/1298 [==============================] - 25s 20ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 1.5726 - val_accuracy: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed7d80ea50>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(input_shape)\n",
    "x = inputs\n",
    "x = encoding_block(128,x)\n",
    "x = encoding_block(32,x)\n",
    "x = encoding_block(2,x)\n",
    "x = layers.Flatten()(x)\n",
    "x = Dense(2,activation = 'softmax')(x)\n",
    "model = keras.Model(inputs,x)\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_df,train_y,batch_size=batch_size,validation_data=(valid_df,valid_y),epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiRouOCg6lnT"
   },
   "source": [
    "#모델 : 트랜스포머 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK5euh-g5sLD"
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9qzPl07dmhi"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6a3YRp9dzgB"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss= root_mean_squared_error,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJMOUD1EeBAG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1239149,
     "status": "error",
     "timestamp": 1657984218448,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "Nn_Q2PISePad",
    "outputId": "4b916aa6-9495-43ff-ae76-3b70a29da7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1298/1298 [==============================] - 17s 12ms/step - loss: 0.6824 - accuracy: 0.5621 - val_loss: 0.6789 - val_accuracy: 0.5770\n",
      "Epoch 2/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6755 - accuracy: 0.5802 - val_loss: 0.6652 - val_accuracy: 0.6102\n",
      "Epoch 3/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6622 - accuracy: 0.6030 - val_loss: 0.6551 - val_accuracy: 0.6233\n",
      "Epoch 4/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6566 - accuracy: 0.6133 - val_loss: 0.6564 - val_accuracy: 0.6287\n",
      "Epoch 5/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6521 - accuracy: 0.6176 - val_loss: 0.6493 - val_accuracy: 0.6248\n",
      "Epoch 6/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6479 - accuracy: 0.6187 - val_loss: 0.6520 - val_accuracy: 0.6202\n",
      "Epoch 7/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6485 - accuracy: 0.6222 - val_loss: 0.7023 - val_accuracy: 0.6125\n",
      "Epoch 8/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.6443 - accuracy: 0.6270 - val_loss: 0.6542 - val_accuracy: 0.6210\n",
      "Epoch 9/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6407 - accuracy: 0.6343 - val_loss: 0.6440 - val_accuracy: 0.6360\n",
      "Epoch 10/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6385 - accuracy: 0.6367 - val_loss: 0.6383 - val_accuracy: 0.6379\n",
      "Epoch 11/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6368 - accuracy: 0.6359 - val_loss: 0.6495 - val_accuracy: 0.6402\n",
      "Epoch 12/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6351 - accuracy: 0.6364 - val_loss: 0.6350 - val_accuracy: 0.6410\n",
      "Epoch 13/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6324 - accuracy: 0.6412 - val_loss: 0.6490 - val_accuracy: 0.6460\n",
      "Epoch 14/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6287 - accuracy: 0.6426 - val_loss: 0.6483 - val_accuracy: 0.6537\n",
      "Epoch 15/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6251 - accuracy: 0.6490 - val_loss: 0.6276 - val_accuracy: 0.6564\n",
      "Epoch 16/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6233 - accuracy: 0.6512 - val_loss: 0.6796 - val_accuracy: 0.6325\n",
      "Epoch 17/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6214 - accuracy: 0.6508 - val_loss: 0.7203 - val_accuracy: 0.6275\n",
      "Epoch 18/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6198 - accuracy: 0.6530 - val_loss: 0.6617 - val_accuracy: 0.6302\n",
      "Epoch 19/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6166 - accuracy: 0.6564 - val_loss: 0.6129 - val_accuracy: 0.6599\n",
      "Epoch 20/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6102 - accuracy: 0.6630 - val_loss: 0.6099 - val_accuracy: 0.6741\n",
      "Epoch 21/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6081 - accuracy: 0.6663 - val_loss: 0.6281 - val_accuracy: 0.6706\n",
      "Epoch 22/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6035 - accuracy: 0.6737 - val_loss: 0.6114 - val_accuracy: 0.6703\n",
      "Epoch 23/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6009 - accuracy: 0.6753 - val_loss: 0.5962 - val_accuracy: 0.6810\n",
      "Epoch 24/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5962 - accuracy: 0.6798 - val_loss: 0.6071 - val_accuracy: 0.6803\n",
      "Epoch 25/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5938 - accuracy: 0.6833 - val_loss: 0.6031 - val_accuracy: 0.6649\n",
      "Epoch 26/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5893 - accuracy: 0.6853 - val_loss: 0.5947 - val_accuracy: 0.6814\n",
      "Epoch 27/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5879 - accuracy: 0.6906 - val_loss: 0.5898 - val_accuracy: 0.6880\n",
      "Epoch 28/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5846 - accuracy: 0.6924 - val_loss: 0.6102 - val_accuracy: 0.6787\n",
      "Epoch 29/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5836 - accuracy: 0.6946 - val_loss: 0.5910 - val_accuracy: 0.6803\n",
      "Epoch 30/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5798 - accuracy: 0.6953 - val_loss: 0.5924 - val_accuracy: 0.6841\n",
      "Epoch 31/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5781 - accuracy: 0.6947 - val_loss: 0.6155 - val_accuracy: 0.6795\n",
      "Epoch 32/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5771 - accuracy: 0.6974 - val_loss: 0.6244 - val_accuracy: 0.6583\n",
      "Epoch 33/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5786 - accuracy: 0.6958 - val_loss: 0.5999 - val_accuracy: 0.6853\n",
      "Epoch 34/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5719 - accuracy: 0.7040 - val_loss: 0.6338 - val_accuracy: 0.6699\n",
      "Epoch 35/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5707 - accuracy: 0.7037 - val_loss: 0.6247 - val_accuracy: 0.6730\n",
      "Epoch 36/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5701 - accuracy: 0.7034 - val_loss: 0.6178 - val_accuracy: 0.6818\n",
      "Epoch 37/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5685 - accuracy: 0.7048 - val_loss: 0.6057 - val_accuracy: 0.6830\n",
      "Epoch 38/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5677 - accuracy: 0.7066 - val_loss: 0.5829 - val_accuracy: 0.6903\n",
      "Epoch 39/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5656 - accuracy: 0.7068 - val_loss: 0.6143 - val_accuracy: 0.6760\n",
      "Epoch 40/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5635 - accuracy: 0.7082 - val_loss: 0.6083 - val_accuracy: 0.6899\n",
      "Epoch 41/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5630 - accuracy: 0.7063 - val_loss: 0.5962 - val_accuracy: 0.6938\n",
      "Epoch 42/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5604 - accuracy: 0.7086 - val_loss: 0.6024 - val_accuracy: 0.6853\n",
      "Epoch 43/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5592 - accuracy: 0.7130 - val_loss: 0.5882 - val_accuracy: 0.6907\n",
      "Epoch 44/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5609 - accuracy: 0.7127 - val_loss: 0.6003 - val_accuracy: 0.6868\n",
      "Epoch 45/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5605 - accuracy: 0.7068 - val_loss: 0.6163 - val_accuracy: 0.6799\n",
      "Epoch 46/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5566 - accuracy: 0.7159 - val_loss: 0.5873 - val_accuracy: 0.6872\n",
      "Epoch 47/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5565 - accuracy: 0.7128 - val_loss: 0.5849 - val_accuracy: 0.6880\n",
      "Epoch 48/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5550 - accuracy: 0.7168 - val_loss: 0.6291 - val_accuracy: 0.6853\n",
      "Epoch 49/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5541 - accuracy: 0.7120 - val_loss: 0.6270 - val_accuracy: 0.6880\n",
      "Epoch 50/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5516 - accuracy: 0.7176 - val_loss: 0.5968 - val_accuracy: 0.6868\n",
      "Epoch 51/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5511 - accuracy: 0.7152 - val_loss: 0.5932 - val_accuracy: 0.6899\n",
      "Epoch 52/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5528 - accuracy: 0.7154 - val_loss: 0.5936 - val_accuracy: 0.6918\n",
      "Epoch 53/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5516 - accuracy: 0.7163 - val_loss: 0.5904 - val_accuracy: 0.6872\n",
      "Epoch 54/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5513 - accuracy: 0.7191 - val_loss: 0.6126 - val_accuracy: 0.6691\n",
      "Epoch 55/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5509 - accuracy: 0.7148 - val_loss: 0.5950 - val_accuracy: 0.6938\n",
      "Epoch 56/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5521 - accuracy: 0.7156 - val_loss: 0.5817 - val_accuracy: 0.6918\n",
      "Epoch 57/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5481 - accuracy: 0.7180 - val_loss: 0.6080 - val_accuracy: 0.6914\n",
      "Epoch 58/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5461 - accuracy: 0.7186 - val_loss: 0.5975 - val_accuracy: 0.6899\n",
      "Epoch 59/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5472 - accuracy: 0.7199 - val_loss: 0.6031 - val_accuracy: 0.6872\n",
      "Epoch 60/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5449 - accuracy: 0.7207 - val_loss: 0.5820 - val_accuracy: 0.6984\n",
      "Epoch 61/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5432 - accuracy: 0.7224 - val_loss: 0.5945 - val_accuracy: 0.6884\n",
      "Epoch 62/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5447 - accuracy: 0.7205 - val_loss: 0.5842 - val_accuracy: 0.6953\n",
      "Epoch 63/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5440 - accuracy: 0.7227 - val_loss: 0.5886 - val_accuracy: 0.6953\n",
      "Epoch 64/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5427 - accuracy: 0.7228 - val_loss: 0.5996 - val_accuracy: 0.6822\n",
      "Epoch 65/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5424 - accuracy: 0.7205 - val_loss: 0.5810 - val_accuracy: 0.6968\n",
      "Epoch 66/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5400 - accuracy: 0.7243 - val_loss: 0.5835 - val_accuracy: 0.6914\n",
      "Epoch 67/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5399 - accuracy: 0.7239 - val_loss: 0.5876 - val_accuracy: 0.6941\n",
      "Epoch 68/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5391 - accuracy: 0.7238 - val_loss: 0.6046 - val_accuracy: 0.6834\n",
      "Epoch 69/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5388 - accuracy: 0.7248 - val_loss: 0.6248 - val_accuracy: 0.6845\n",
      "Epoch 70/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5358 - accuracy: 0.7271 - val_loss: 0.5878 - val_accuracy: 0.6995\n",
      "Epoch 71/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5355 - accuracy: 0.7249 - val_loss: 0.5993 - val_accuracy: 0.6764\n",
      "Epoch 72/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5384 - accuracy: 0.7254 - val_loss: 0.5939 - val_accuracy: 0.6857\n",
      "Epoch 73/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5361 - accuracy: 0.7292 - val_loss: 0.6071 - val_accuracy: 0.6780\n",
      "Epoch 74/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5358 - accuracy: 0.7266 - val_loss: 0.6105 - val_accuracy: 0.6949\n",
      "Epoch 75/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5357 - accuracy: 0.7280 - val_loss: 0.5946 - val_accuracy: 0.6888\n",
      "Epoch 76/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5317 - accuracy: 0.7311 - val_loss: 0.6054 - val_accuracy: 0.6818\n",
      "Epoch 77/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5317 - accuracy: 0.7284 - val_loss: 0.5924 - val_accuracy: 0.6822\n",
      "Epoch 78/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5316 - accuracy: 0.7291 - val_loss: 0.6098 - val_accuracy: 0.6780\n",
      "Epoch 79/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5314 - accuracy: 0.7317 - val_loss: 0.5903 - val_accuracy: 0.6965\n",
      "Epoch 80/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5310 - accuracy: 0.7296 - val_loss: 0.5867 - val_accuracy: 0.6945\n",
      "Epoch 81/100\n",
      " 754/1298 [================>.............] - ETA: 6s - loss: 0.5292 - accuracy: 0.7333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-d0be928aa256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_df,\n",
    "    train_y,\n",
    "    validation_data=(valid_df,valid_y),\n",
    "    epochs=100,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV4rHf3Gj3lZ"
   },
   "source": [
    "# 모델 : 트랜스포머 + 포지셔널 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKfEyUQXkabc"
   },
   "outputs": [],
   "source": [
    "!pip install keras-nlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNE82CX8kL6s"
   },
   "outputs": [],
   "source": [
    "from  keras_nlp.layers import SinePositionEncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wk4REu9j5jq"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    inputs = SinePositionEncoding()(inputs)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1549812,
     "status": "error",
     "timestamp": 1657986335921,
     "user": {
      "displayName": "Hun",
      "userId": "07972583068794560208"
     },
     "user_tz": -540
    },
    "id": "FVhOQH9gmBWi",
    "outputId": "beb9d2fc-ea61-4cf1-9f78-22c25c981c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1298/1298 [==============================] - 18s 12ms/step - loss: 0.7020 - accuracy: 0.4988 - val_loss: 0.6944 - val_accuracy: 0.5069\n",
      "Epoch 2/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6945 - accuracy: 0.5089 - val_loss: 0.6928 - val_accuracy: 0.5204\n",
      "Epoch 3/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.6907 - accuracy: 0.5264 - val_loss: 0.6912 - val_accuracy: 0.5220\n",
      "Epoch 4/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6886 - accuracy: 0.5395 - val_loss: 0.6880 - val_accuracy: 0.5362\n",
      "Epoch 5/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6836 - accuracy: 0.5579 - val_loss: 0.6836 - val_accuracy: 0.5474\n",
      "Epoch 6/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6805 - accuracy: 0.5689 - val_loss: 0.6796 - val_accuracy: 0.5743\n",
      "Epoch 7/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6747 - accuracy: 0.5849 - val_loss: 0.6652 - val_accuracy: 0.6133\n",
      "Epoch 8/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6641 - accuracy: 0.6016 - val_loss: 0.6575 - val_accuracy: 0.6152\n",
      "Epoch 9/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.6584 - accuracy: 0.6085 - val_loss: 0.6566 - val_accuracy: 0.6194\n",
      "Epoch 10/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6551 - accuracy: 0.6106 - val_loss: 0.6546 - val_accuracy: 0.6190\n",
      "Epoch 11/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6518 - accuracy: 0.6177 - val_loss: 0.6635 - val_accuracy: 0.6213\n",
      "Epoch 12/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6492 - accuracy: 0.6205 - val_loss: 0.6527 - val_accuracy: 0.6206\n",
      "Epoch 13/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6475 - accuracy: 0.6220 - val_loss: 0.6580 - val_accuracy: 0.6198\n",
      "Epoch 14/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6449 - accuracy: 0.6291 - val_loss: 0.6803 - val_accuracy: 0.6144\n",
      "Epoch 15/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6440 - accuracy: 0.6273 - val_loss: 0.7073 - val_accuracy: 0.5998\n",
      "Epoch 16/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6419 - accuracy: 0.6310 - val_loss: 0.6936 - val_accuracy: 0.6156\n",
      "Epoch 17/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6420 - accuracy: 0.6323 - val_loss: 0.6566 - val_accuracy: 0.6244\n",
      "Epoch 18/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6396 - accuracy: 0.6334 - val_loss: 0.6626 - val_accuracy: 0.6163\n",
      "Epoch 19/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6388 - accuracy: 0.6348 - val_loss: 0.7015 - val_accuracy: 0.5894\n",
      "Epoch 20/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6370 - accuracy: 0.6367 - val_loss: 0.6792 - val_accuracy: 0.6117\n",
      "Epoch 21/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6335 - accuracy: 0.6410 - val_loss: 0.6584 - val_accuracy: 0.6244\n",
      "Epoch 22/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.6335 - accuracy: 0.6381 - val_loss: 0.6620 - val_accuracy: 0.6221\n",
      "Epoch 23/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6314 - accuracy: 0.6417 - val_loss: 0.6458 - val_accuracy: 0.6287\n",
      "Epoch 24/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6311 - accuracy: 0.6426 - val_loss: 0.6851 - val_accuracy: 0.6248\n",
      "Epoch 25/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6285 - accuracy: 0.6468 - val_loss: 0.6544 - val_accuracy: 0.6148\n",
      "Epoch 26/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6254 - accuracy: 0.6462 - val_loss: 0.6918 - val_accuracy: 0.6244\n",
      "Epoch 27/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6239 - accuracy: 0.6497 - val_loss: 0.6501 - val_accuracy: 0.6456\n",
      "Epoch 28/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.6225 - accuracy: 0.6504 - val_loss: 0.6555 - val_accuracy: 0.6383\n",
      "Epoch 29/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6190 - accuracy: 0.6569 - val_loss: 0.6957 - val_accuracy: 0.6125\n",
      "Epoch 30/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6163 - accuracy: 0.6545 - val_loss: 0.6944 - val_accuracy: 0.6391\n",
      "Epoch 31/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.6124 - accuracy: 0.6643 - val_loss: 0.6425 - val_accuracy: 0.6572\n",
      "Epoch 32/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6065 - accuracy: 0.6699 - val_loss: 0.7008 - val_accuracy: 0.6518\n",
      "Epoch 33/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.6043 - accuracy: 0.6714 - val_loss: 0.6340 - val_accuracy: 0.6633\n",
      "Epoch 34/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.6008 - accuracy: 0.6740 - val_loss: 0.6621 - val_accuracy: 0.6614\n",
      "Epoch 35/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5976 - accuracy: 0.6797 - val_loss: 0.6205 - val_accuracy: 0.6703\n",
      "Epoch 36/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5965 - accuracy: 0.6813 - val_loss: 0.6148 - val_accuracy: 0.6699\n",
      "Epoch 37/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5923 - accuracy: 0.6813 - val_loss: 0.6228 - val_accuracy: 0.6691\n",
      "Epoch 38/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5909 - accuracy: 0.6831 - val_loss: 0.6150 - val_accuracy: 0.6722\n",
      "Epoch 39/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5902 - accuracy: 0.6846 - val_loss: 0.6089 - val_accuracy: 0.6706\n",
      "Epoch 40/100\n",
      "1298/1298 [==============================] - 19s 14ms/step - loss: 0.5867 - accuracy: 0.6872 - val_loss: 0.6202 - val_accuracy: 0.6706\n",
      "Epoch 41/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5860 - accuracy: 0.6872 - val_loss: 0.6465 - val_accuracy: 0.6772\n",
      "Epoch 42/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5848 - accuracy: 0.6896 - val_loss: 0.6350 - val_accuracy: 0.6726\n",
      "Epoch 43/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5836 - accuracy: 0.6913 - val_loss: 0.6209 - val_accuracy: 0.6695\n",
      "Epoch 44/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5835 - accuracy: 0.6894 - val_loss: 0.6030 - val_accuracy: 0.6710\n",
      "Epoch 45/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5822 - accuracy: 0.6912 - val_loss: 0.6205 - val_accuracy: 0.6703\n",
      "Epoch 46/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5816 - accuracy: 0.6934 - val_loss: 0.6127 - val_accuracy: 0.6799\n",
      "Epoch 47/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5793 - accuracy: 0.6930 - val_loss: 0.6259 - val_accuracy: 0.6780\n",
      "Epoch 48/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5785 - accuracy: 0.6958 - val_loss: 0.6401 - val_accuracy: 0.6730\n",
      "Epoch 49/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5765 - accuracy: 0.6955 - val_loss: 0.6075 - val_accuracy: 0.6837\n",
      "Epoch 50/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5768 - accuracy: 0.6966 - val_loss: 0.6224 - val_accuracy: 0.6810\n",
      "Epoch 51/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5746 - accuracy: 0.6979 - val_loss: 0.6187 - val_accuracy: 0.6822\n",
      "Epoch 52/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5731 - accuracy: 0.6967 - val_loss: 0.6090 - val_accuracy: 0.6791\n",
      "Epoch 53/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5729 - accuracy: 0.6952 - val_loss: 0.6164 - val_accuracy: 0.6772\n",
      "Epoch 54/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5738 - accuracy: 0.6991 - val_loss: 0.6425 - val_accuracy: 0.6722\n",
      "Epoch 55/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5702 - accuracy: 0.7036 - val_loss: 0.6163 - val_accuracy: 0.6726\n",
      "Epoch 56/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5714 - accuracy: 0.7010 - val_loss: 0.6165 - val_accuracy: 0.6668\n",
      "Epoch 57/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5715 - accuracy: 0.7015 - val_loss: 0.6097 - val_accuracy: 0.6737\n",
      "Epoch 58/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5692 - accuracy: 0.7037 - val_loss: 0.5965 - val_accuracy: 0.6876\n",
      "Epoch 59/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5678 - accuracy: 0.7049 - val_loss: 0.6105 - val_accuracy: 0.6749\n",
      "Epoch 60/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5676 - accuracy: 0.7051 - val_loss: 0.6153 - val_accuracy: 0.6718\n",
      "Epoch 61/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5674 - accuracy: 0.7043 - val_loss: 0.6278 - val_accuracy: 0.6791\n",
      "Epoch 62/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5659 - accuracy: 0.7034 - val_loss: 0.6232 - val_accuracy: 0.6787\n",
      "Epoch 63/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5644 - accuracy: 0.7035 - val_loss: 0.6038 - val_accuracy: 0.6830\n",
      "Epoch 64/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5634 - accuracy: 0.7060 - val_loss: 0.6135 - val_accuracy: 0.6745\n",
      "Epoch 65/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5637 - accuracy: 0.7072 - val_loss: 0.6375 - val_accuracy: 0.6683\n",
      "Epoch 66/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5633 - accuracy: 0.7101 - val_loss: 0.6018 - val_accuracy: 0.6807\n",
      "Epoch 67/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5614 - accuracy: 0.7098 - val_loss: 0.6224 - val_accuracy: 0.6764\n",
      "Epoch 68/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5631 - accuracy: 0.7089 - val_loss: 0.6173 - val_accuracy: 0.6568\n",
      "Epoch 69/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5600 - accuracy: 0.7129 - val_loss: 0.6291 - val_accuracy: 0.6683\n",
      "Epoch 70/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5578 - accuracy: 0.7113 - val_loss: 0.6031 - val_accuracy: 0.6706\n",
      "Epoch 71/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5589 - accuracy: 0.7103 - val_loss: 0.6195 - val_accuracy: 0.6733\n",
      "Epoch 72/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5555 - accuracy: 0.7137 - val_loss: 0.6115 - val_accuracy: 0.6664\n",
      "Epoch 73/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5554 - accuracy: 0.7120 - val_loss: 0.6245 - val_accuracy: 0.6787\n",
      "Epoch 74/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5558 - accuracy: 0.7145 - val_loss: 0.6091 - val_accuracy: 0.6710\n",
      "Epoch 75/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5550 - accuracy: 0.7148 - val_loss: 0.6194 - val_accuracy: 0.6726\n",
      "Epoch 76/100\n",
      "1298/1298 [==============================] - 23s 17ms/step - loss: 0.5551 - accuracy: 0.7123 - val_loss: 0.6217 - val_accuracy: 0.6730\n",
      "Epoch 77/100\n",
      "1298/1298 [==============================] - 21s 16ms/step - loss: 0.5520 - accuracy: 0.7158 - val_loss: 0.6433 - val_accuracy: 0.6703\n",
      "Epoch 78/100\n",
      "1298/1298 [==============================] - 20s 16ms/step - loss: 0.5525 - accuracy: 0.7163 - val_loss: 0.6086 - val_accuracy: 0.6757\n",
      "Epoch 79/100\n",
      "1298/1298 [==============================] - 20s 15ms/step - loss: 0.5506 - accuracy: 0.7192 - val_loss: 0.6099 - val_accuracy: 0.6791\n",
      "Epoch 80/100\n",
      "1298/1298 [==============================] - 19s 15ms/step - loss: 0.5505 - accuracy: 0.7174 - val_loss: 0.6241 - val_accuracy: 0.6764\n",
      "Epoch 81/100\n",
      "1298/1298 [==============================] - 22s 17ms/step - loss: 0.5492 - accuracy: 0.7196 - val_loss: 0.6245 - val_accuracy: 0.6683\n",
      "Epoch 82/100\n",
      "1298/1298 [==============================] - 22s 17ms/step - loss: 0.5491 - accuracy: 0.7192 - val_loss: 0.6237 - val_accuracy: 0.6660\n",
      "Epoch 83/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5488 - accuracy: 0.7183 - val_loss: 0.6553 - val_accuracy: 0.6737\n",
      "Epoch 84/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5477 - accuracy: 0.7166 - val_loss: 0.6475 - val_accuracy: 0.6714\n",
      "Epoch 85/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5432 - accuracy: 0.7218 - val_loss: 0.6279 - val_accuracy: 0.6726\n",
      "Epoch 86/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5445 - accuracy: 0.7201 - val_loss: 0.6370 - val_accuracy: 0.6718\n",
      "Epoch 87/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5413 - accuracy: 0.7285 - val_loss: 0.6236 - val_accuracy: 0.6768\n",
      "Epoch 88/100\n",
      "1298/1298 [==============================] - 16s 13ms/step - loss: 0.5421 - accuracy: 0.7232 - val_loss: 0.6131 - val_accuracy: 0.6726\n",
      "Epoch 89/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5427 - accuracy: 0.7249 - val_loss: 0.6357 - val_accuracy: 0.6753\n",
      "Epoch 90/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5416 - accuracy: 0.7250 - val_loss: 0.6651 - val_accuracy: 0.6737\n",
      "Epoch 91/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5385 - accuracy: 0.7272 - val_loss: 0.6375 - val_accuracy: 0.6814\n",
      "Epoch 92/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5391 - accuracy: 0.7281 - val_loss: 0.6178 - val_accuracy: 0.6757\n",
      "Epoch 93/100\n",
      "1298/1298 [==============================] - 15s 12ms/step - loss: 0.5379 - accuracy: 0.7243 - val_loss: 0.6221 - val_accuracy: 0.6660\n",
      "Epoch 94/100\n",
      "1298/1298 [==============================] - 16s 12ms/step - loss: 0.5354 - accuracy: 0.7296 - val_loss: 0.6417 - val_accuracy: 0.6818\n",
      "Epoch 95/100\n",
      "1298/1298 [==============================] - 17s 13ms/step - loss: 0.5348 - accuracy: 0.7279 - val_loss: 0.7044 - val_accuracy: 0.6787\n",
      "Epoch 96/100\n",
      " 972/1298 [=====================>........] - ETA: 3s - loss: 0.5360 - accuracy: 0.7286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-d0be928aa256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_df,\n",
    "    train_y,\n",
    "    validation_data=(valid_df,valid_y),\n",
    "    epochs=100,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "\n",
    "from tensorflow import keras"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM4yleeaT3OEMejNnHEgyFq",
   "collapsed_sections": [],
   "mount_file_id": "1Cmm6JDo7WP8xKZLCw93lfJvLIhbHND7K",
   "name": "4회차.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
